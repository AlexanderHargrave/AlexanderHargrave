{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderHargrave/EvidenceDetection_Unsupervised/blob/main/Group_13_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evidence detection using Unsupervised or traditional approach**\n",
        "\n",
        "This notebook approaches the task of evidence detection using a Conditional Random Field(CRF) approach. It initial cleans, tokenize and get the POS tags of the text then extracts the features used for the CRF model. These features are:\n",
        "\n",
        "\n",
        "*   The words in the 'claim' section\n",
        "*   The POS tags for those words\n",
        "*   The minimum distance between tokens in claim and evidence\n",
        "*   The words in both 'claim' and 'evidence' section\n",
        "*   Using Word2Vec model to provide contextual information for words\n",
        "\n",
        "These extracted features are then used to develop the CRF model where the model is run using the sklearn_crfsuite library. This model is then used to predict for both the development and testing set where the development set uses functions from sklearn.metrics which are accuracy_score and classification_report to present the results of the predictions of the development set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQyVuJChRo88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "import gensim.downloader as api\n",
        "import string\n",
        "\n",
        "# Tokenization and POS Tagging function\n",
        "def tokenize_and_tag(text):\n",
        "    text = str(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return pos_tags\n",
        "# Feature extraction function\n",
        "def extract_features(data, word_embeddings_model, dev_test):\n",
        "    X = []\n",
        "    y = []\n",
        "    # Going through every row\n",
        "    for _, row in data.iterrows():\n",
        "        claim_tokens = tokenize_and_tag(row['Claim'])\n",
        "        evidence_tokens = tokenize_and_tag(row['Evidence'])\n",
        "        # Setting up the features to be extracted from the text\n",
        "        features = []\n",
        "\n",
        "        for word, pos_tag in claim_tokens:\n",
        "            min_distance = min([abs(claim_tokens.index((word, pos_tag)) - evidence_tokens.index((e_word, e_pos_tag))) for e_word, e_pos_tag in evidence_tokens] or [-1])\n",
        "            word_embedding = None\n",
        "\n",
        "            # Check if the word is in the Word2Vec vocabulary\n",
        "            if word in word_embeddings_model.wv:\n",
        "                word_embedding = word_embeddings_model.wv[word]\n",
        "            # The features to be extracted\n",
        "            features.extend([{\n",
        "                f'word={word}',\n",
        "                f'pos_tag={pos_tag}',\n",
        "                f'min_distance={min_distance}',\n",
        "                f'in_evidence={word.lower() in [e_word.lower() for e_word, _ in evidence_tokens]}',\n",
        "                f'word_embedding={word_embedding}'\n",
        "            }])\n",
        "\n",
        "        X.append(features)\n",
        "        # Used to determine the label correctly from training and development data set\n",
        "        if dev_test == 'dev':\n",
        "          y.append(['1' if row['label'] == 1 else '0' for i in claim_tokens])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qFtZL8RAy3a0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model\n",
        "This is used to load the development and training data sets where a word2vec model is also generated, the features are then extracted from the training data set which is used to generate the CRF model. The parameters of the word2vec model and the CRF model are determiend through testing using the development data set to help increase accuracy and other metrics."
      ],
      "metadata": {
        "id": "IQEnzKwMP2zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "dev_data = pd.read_csv('dev.csv')\n",
        "\n",
        "# Preprocess and extract features\n",
        "all_texts = train_data['Claim'].tolist() + train_data['Evidence'].tolist()\n",
        "tokenized_texts = [word_tokenize(text) for text in all_texts]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=2, workers=multiprocessing.cpu_count())\n",
        "\n",
        "# Save the trained Word2Vec model for future use\n",
        "word2vec_model.save('word2vec_model.bin')\n",
        "\n",
        "# Preprocess and extract features with the trained Word2Vec model\n",
        "X_train, y_train = extract_features(train_data, word2vec_model, 'dev')\n",
        "crf = CRF(algorithm='lbfgs',linesearch='MoreThuente',min_freq = 1, c1 = 0.1, c2 = 0.9, max_iterations=85, all_possible_transitions=True)\n",
        "try:\n",
        "    crf.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "nm6chXSaRIGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "This section is used to run the trained CRF model generated previously on the development data set to predict the correct labels, the model gets the accuracy and other metrics of the prediction and displays them."
      ],
      "metadata": {
        "id": "8gPzAdCnQOIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "X_dev, y_dev = extract_features(dev_data, word2vec_model, 'dev')\n",
        "y_pred = crf.predict(X_dev)\n",
        "y_pred = [sublist[0] for sublist in y_pred]\n",
        "y_dev = [sublist[0] for sublist in y_dev]\n",
        "accuracy = accuracy_score(y_dev, y_pred)\n",
        "# Outputs the accuracy and the classification report which contains the precision, recall and f1_scores\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_dev, y_pred))\n"
      ],
      "metadata": {
        "id": "8nO0KIJ52Zve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec28233-0721-4918-f6a8-0d8fdbaaf394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8044211947350658\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87      4327\n",
            "           1       0.67      0.54      0.60      1599\n",
            "\n",
            "    accuracy                           0.80      5926\n",
            "   macro avg       0.76      0.72      0.73      5926\n",
            "weighted avg       0.80      0.80      0.80      5926\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demoing the test data\n",
        "\n",
        "This sections downloads the testing data where the previously generated CRF model is run after extracting the correct features from the testing data. The predicitons are then stored on a Data Frame which is then saved as a CSV file and outputted in a presentable format."
      ],
      "metadata": {
        "id": "y4VzNku7Qkhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data and used to predict the testing data and generate the csv file\n",
        "test_data = pd.read_csv('test.csv')\n",
        "X_test, y_test = extract_features(test_data, word2vec_model, 'train')\n",
        "y_pred = crf.predict(X_test)\n",
        "y_pred = [sublist[0] for sublist in y_pred]\n",
        "result_df = pd.DataFrame(y_pred, columns = ['prediction'])\n",
        "result_df.to_csv('./Group_13_A.csv', index = False, header = True)\n"
      ],
      "metadata": {
        "id": "y3fliO5tuKZW"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}